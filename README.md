# Ollama Flask Bot

Este proyecto es un bot basado en Flask que se comunica con una instancia local de Ollama para generar respuestas a consultas. El bot recibe consultas a trav√©s de solicitudes POST y devuelve la respuesta generada por Ollama.

## Requisitos

- Python 3.x
- Flask
- Requests

## Instalaci√≥n

1. **Clona el repositorio:**

   ```bash
   git clone https://github.com/tu_usuario/ollama-flask-bot.git
   cd ollama-flask-bot
   ```

2. **Crea un entorno virtual (opcional pero recomendado):**

   ```bash
   python -m venv venv
   source venv/bin/activate  # En Windows: venv\Scripts\activate
   ```

3. **Instala las dependencias:**

   ```bash
   pip install -r requirements.txt
   ```

4. **Configura los par√°metros en `config.json`:**

   Aseg√∫rate de que los valores en `config.json` sean correctos antes de ejecutar el bot. Aqu√≠ est√° la configuraci√≥n final:

   ```json
   {
     "ollama_url": "http://127.0.0.1:11434",
     "api_endpoint": "/api/generate",
     "flask_host": "0.0.0.0",
     "flask_port": 5000,
     "model_config": {
       "model": "gemma2:2b",
       "stream": false,
       "temp": 0.3,
       "seed": -1,
       "role": "assistant"
     }
   }
   ```

## Ejecuci√≥n

1. **Ejecuta la aplicaci√≥n Flask:**

   ```bash
   python src/bot.py
   ```

   Esto iniciar√° el servidor Flask en `http://0.0.0.0:5000`, que estar√° listo para recibir solicitudes POST en la ruta `/consultar`.

2. **Hacer una solicitud POST:**

   Puedes utilizar Postman, `curl` u otro cliente HTTP para hacer una solicitud POST al bot. Aqu√≠ tienes un ejemplo utilizando `curl`:

   ```bash
   curl -X POST http://localhost:5000/consultar -H "Content-Type: application/json" -d '{"consulta": "hola, buenos dias"}'
   ```

   La respuesta ser√° algo similar a:

   ```json
   {
     "response": "¬°Hola! ¬°Buenos d√≠as a ti tambi√©n! üòä  ¬øC√≥mo est√°s?"
   }
   ```

## Estructura del Proyecto

```
üì¶src
 ‚î£ üìúbot.py           # C√≥digo principal de la aplicaci√≥n Flask
 ‚îó üìúconfig.json      # Configuraci√≥n del bot y la conexi√≥n a Ollama
```

## Personalizaci√≥n

- **Configuraci√≥n del Modelo**: Puedes ajustar los par√°metros del modelo en el archivo `config.json` bajo la clave `"model_config"`.
- **Endpoint y URL de Ollama**: Aseg√∫rate de que la URL de Ollama y el endpoint est√©n correctamente configurados en el archivo `config.json`.

## Contribuciones

Las contribuciones son bienvenidas. Si encuentras alg√∫n problema
